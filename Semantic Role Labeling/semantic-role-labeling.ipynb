{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Role Labeling\n",
    "\n",
    "NLP - Spring Semester of 2024 at University of Tehran - CA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "We will implement Semantic Role Labeling LSTM, GRU, and Encoder-Decoder models. The role labels that we use are as follows:\n",
    "\n",
    "- `Arg0`: Agent\n",
    "- `Arg1`: Patient\n",
    "- `Arg2`: Instrument\n",
    "- `ArgM-LOC`: Location of the verb\n",
    "- `ArgM-TMP`: Time of the verb\n",
    "\n",
    "Let's take a look into the datasets before we proceed to the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We have three files: [`train.json`](./data/train.json), [`valid.json`](./data/valid.json), and [`test.json`](./data/test.json).\n",
    "\n",
    "These three have four fields `text`, which is the input sentence, `verb_index`, which is zero-based index of the verb in the sentence, `srl_label` which is the label for each word in the sentence, and `word_indices`, which is the index of each word in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first write a function that simply reads the json file and store each list object as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_name: str) -> pd.DataFrame:\n",
    "    with open(file_name, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_json('./data/train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we need a function to transform the given `srl_frames` into numerical values. We don't really have to do anything but call the `map` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRL_TO_NUM = {'O': 0,  'B-ARG0': 1,  'I-ARG0': 2,  'B-ARG1': 3,  'I-ARG1': 4,  'B-ARG2': 5,\n",
    "              'I-ARG2': 6,  'B-ARGM-LOC': 7,  'I-ARGM': 8,  'B-ARGM-TMP': 9, 'I-ARGM-TMP': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll have to write a function that pads the sentences to make them have equal length with each other. For this purpose we'll add `'[pad]'` to the sentences until they reach the size of the biggest sentence. we'll also fill the other columns with other proper values corresponding to the pad token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data_sequence(sequences: list[list[str]], pad_token: str) -> pd.DataFrame:\n",
    "    max_len = max(sequences, key=len)\n",
    "    result = [seq + [pad_token] * (max_len - len(seq)) for seq in sequences]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the `Vocab` class. This class is simply a place to hold vocabulary and provide some tools to work with. We'll add its methods along the way in different code cells by inheriting `Vocab` from itself.\n",
    "The first method with be constructor. The constructor takes an optional argument and sets the `word2id` which is just a map from words to indexes. It'll also add four default values in the vocabulary whether they are in the given `word2id` or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    PAD:   str = '[PAD]'\n",
    "    START: str = '[START]'\n",
    "    END:   str = '[END]'\n",
    "    UNK:   str = '[UNKOWN]'\n",
    "\n",
    "    def __init__(self, word2id: dict[str, int] = None, default_tokens: bool = True) -> None:\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "        \n",
    "        if default_tokens:\n",
    "            self._add_default_tokens()\n",
    "\n",
    "        self.default_tokens = default_tokens\n",
    "        self.id2word = {index: word for word, index in self.word2id.items()}\n",
    "    \n",
    "    def _add_default_tokens(self) -> None:\n",
    "        non_existent_default_tokens = dict()\n",
    "        if self.PAD not in self.word2id:\n",
    "            non_existent_default_tokens[self.PAD] = 0\n",
    "        if self.START not in self.word2id:\n",
    "            non_existent_default_tokens[self.START] = 1\n",
    "        if self.END not in self.word2id:\n",
    "            non_existent_default_tokens[self.END] = 2\n",
    "        if self.UNK not in self.word2id:\n",
    "            non_existent_default_tokens[self.UNK] = 3\n",
    "        \n",
    "        self.word2id = {word: index + len(non_existent_default_tokens) for word, index in self.word2id.items()}\n",
    "        self.word2id.update(non_existent_default_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step would be implementing the magic methods for this class. We'll only use the `__getitem__` and `__len__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def __getitem__(self, word: str) -> int:\n",
    "        unknown_index = 0\n",
    "        if self.default_tokens:\n",
    "            unknown_index = self.word2id[self.UNK]\n",
    "        return self.word2id.get(word, unknown_index)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `add` method will simply add a new word to the vocabulary. It'll also return its index in the vocab, which is equal to the current length of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def add(self, word: str) -> int:\n",
    "        assert word not in self.word2id\n",
    "\n",
    "        index = len(self.word2id)\n",
    "        self.word2id[word] = index\n",
    "        self.id2word[index] = word\n",
    "        return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `words2indices` takes a list of sentences, which are a list of tokens, and returns a list of indices (it's actually a list of list of numbers) corresponding to the indices of each word in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def words2indices(self, sentences: list[list[str]]) -> list[list[int]]:\n",
    "        return [\n",
    "            [self[word] for word in sentence] for sentence in sentences\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the reverse of the `words2indices` as well. That would be the `indices2word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def indices2words(self, sentences_indices: list[list[int]]) -> list[list[str]]:\n",
    "        return [\n",
    "            [self.id2word.get(index, self.UNK) for index in indices] for indices in sentences_indices\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_input_tensor` will take a list of sentences, which are a list of tokens, and pad them to have equal length. It'll then return the tensor of word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def to_input_tensor(self, sentences: list[list[str]]) -> list[list[int]]:\n",
    "        max_len = max([len(sentence) for sentence in sentences])\n",
    "        padded_sentences = [sentence + [self.PAD] * (max_len - len(sentence)) for sentence in sentences]\n",
    "        return self.words2indices(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for the static function, `from_corpus`. This function will take a corpus, which is a list of list of words, and then create the vocabulary class based on it. It also takes some arguments which are described below:\n",
    "\n",
    "- `corpus`: This is a list of sentences to create the corpus from.\n",
    "- `size`: This is the maximum number of unique words allowed in the vocabulary.\n",
    "- `remove_frac`: It tells us what fraction of the less frequent words should be removed.\n",
    "- `freq_cutoff`: This causes the words that are repeated less than the cutoff to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(Vocab):\n",
    "    def from_corpus(corpus: list[list[str]], size: int, remove_frac: float, freq_cutoff: float) -> Vocab:\n",
    "        word_freq = {}\n",
    "        for sentence in corpus:\n",
    "            if len(word_freq) > size:\n",
    "                break\n",
    "            for word in sentence:\n",
    "                if word in word_freq:\n",
    "                    word_freq[word] += 1\n",
    "                else:\n",
    "                    word_freq[word] = 1\n",
    "                if len(word_freq) > size:\n",
    "                    break\n",
    "\n",
    "        word_freq_list = sorted(word_freq.items(), key=lambda x: x[1])\n",
    "        word_freq_list = list(filter(lambda x: x[1] > freq_cutoff, word_freq_list))\n",
    "        word_freq_list = word_freq_list[int(len(word_freq_list) * remove_frac):]\n",
    "\n",
    "        vocab = Vocab()\n",
    "        for word, _ in word_freq_list:\n",
    "            vocab.add(word)\n",
    "        \n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. LSTM Encoder Model\n",
    "\n",
    "In this section we'll implement and LSTM model using the pytorch library. Our model will have an embedding layer connected to the LSTM. We will then concatenate the hidden layers from each step and pass them to a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2000\n",
    "REMOVE_FRAC = 0.3\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 64\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCH_COUNT = 50\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSemanticRoleLabeler(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, labels_count):\n",
    "        super(LSTMSemanticRoleLabeler, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, labels_count)\n",
    "\n",
    "    def forward(self, sentence, verb_indices):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        \n",
    "        verb_hidden_states = lstm_out[torch.arange(lstm_out.size(0)), verb_indices]\n",
    "        verb_hidden_states_expanded = verb_hidden_states.unsqueeze(1).expand(-1, lstm_out.size(1), -1)\n",
    "        concatenated_states = torch.cat((lstm_out, verb_hidden_states_expanded), dim=2)\n",
    "\n",
    "        label_space = self.hidden2label(concatenated_states)\n",
    "        label_scores = F.log_softmax(label_space, dim=1)\n",
    "        return label_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to training and testing sections, we need to prepare our data. We will first read the vocabulary and then transform the train data into their corresponding indices. We will then change the tags to their mapped indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLDataSet(data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, text_vocab: Vocab, labels_vocab: Vocab):\n",
    "        self.text_vocab: Vocab = text_vocab\n",
    "        self.text = self.text_vocab.to_input_tensor(df['text'].tolist())\n",
    "        \n",
    "        self.labels_vocab: Vocab = labels_vocab\n",
    "        self.labels = self.labels_vocab.to_input_tensor(df['srl_frames'].tolist())\n",
    "\n",
    "        self.verb_indices =  df['verb_index'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.tensor, torch.tensor]:\n",
    "        return torch.tensor(self.text[index]), self.verb_indices[index], torch.tensor(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vocab = Vocab.from_corpus(train_data['text'].tolist(), VOCAB_SIZE, REMOVE_FRAC, 0)\n",
    "labels_vocab = Vocab(SRL_TO_NUM, False)\n",
    "\n",
    "dataset = SRLDataSet(train_data, train_text_vocab, labels_vocab)\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look into the data batch. Note the `[UNKNOWN]`s that happened due to the `remove_frac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, _, train_labels = next(iter(data_loader))\n",
    "print(f'Sentences batch shape: {train_sentences.size()}')\n",
    "print(f'Labels batch shape: {train_labels.size()}')\n",
    "\n",
    "print(dataset.text_vocab.indices2words([train_sentences[0].tolist()]))\n",
    "print(dataset.labels_vocab.indices2words([train_labels[0].tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to transform data and labels into correct indexes, let's proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSemanticRoleLabeler(EMBEDDING_DIM, HIDDEN_DIM, len(dataset.text_vocab), len(SRL_TO_NUM))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the scores before training the model. The output scores will be the log of the probability distribution given by the softmax layer. Note that there will be an output of the shape [50, 11]. We have 50 rows because this is the output of each time the model was run over the sequence of 50 words. Each row has 11 columns for the 11 label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_input, verb_index, _ = next(iter(data_loader))\n",
    "    labels_scores = model(x_input[0].unsqueeze(0), verb_index[0].unsqueeze(0))\n",
    "    print(f'Shape of the scores: {labels_scores.size()}\\n', labels_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, optimizer, data_loader) -> tuple[list, list]:\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCH_COUNT):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "    \n",
    "        for x_batch, verb_index, y_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            label_scores = model(x_batch, verb_index)\n",
    "    \n",
    "            loss = loss_function(label_scores.view(-1, 11), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            predicted = torch.argmax(label_scores, dim=2)\n",
    "            correct_predictions += (predicted == y_batch).sum().item()\n",
    "            total_samples += y_batch.numel()\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(data_loader)\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "    \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "    \n",
    "        print(f'Epoch {epoch} loss: {epoch_loss}')\n",
    "        print(f'Epoch {epoch} accuracy: {epoch_accuracy}')\n",
    "    \n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies = train_model(model, loss_function, optimizer, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1_score(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, verb_index, y in data_loader:\n",
    "            current_predictions = model(x, verb_index)[0]\n",
    "            current_predictions = torch.argmax(current_predictions, dim=1)\n",
    "            \n",
    "            all_predictions.extend(current_predictions.view(-1).numpy())\n",
    "            all_labels.extend(y.view(-1).numpy())\n",
    "    \n",
    "    return f1_score(all_labels, all_predictions, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_json('./data/train.json')\n",
    "test_dataset = SRLDataSet(test_data, train_text_vocab, labels_vocab)\n",
    "test_data_loader = data.DataLoader(test_dataset)\n",
    "\n",
    "print(f'F1 score: {calc_f1_score(model, test_data_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model is functioning poorly in the F1 score. This is because the data has bias on some classes that occur more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_count = Counter(list(chain.from_iterable(train_data['srl_frames'].tolist())))\n",
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label 'O' has occurred way more than any other class. This causes the model to learn these more frequent labels better and act poorly on others. In order to make this situation better we can pass the inverse frequency as the class weights to the loss function. We'll continue to do this with the other methods in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels_vocab.to_input_tensor(train_data['srl_frames'].tolist())\n",
    "train_labels_flat = list(chain.from_iterable(train_labels))\n",
    "\n",
    "train_labels_freq = Counter(train_labels_flat)\n",
    "train_labels_freq = {label: train_labels_freq.get(label, 1) for label in SRL_TO_NUM.values()}\n",
    "\n",
    "class_weights = {label: 1/train_labels_freq[label] for label in SRL_TO_NUM.values()}.items()\n",
    "class_weights = sorted(class_weights, key=lambda x: x[0])\n",
    "class_weights = [weight for _, weight in class_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSemanticRoleLabeler(EMBEDDING_DIM, HIDDEN_DIM, len(dataset.text_vocab), len(SRL_TO_NUM))\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "_, _ = train_model(model, loss_function, optimizer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_json('./data/train.json')\n",
    "test_dataset = SRLDataSet(test_data, train_text_vocab, labels_vocab)\n",
    "test_data_loader = data.DataLoader(test_dataset)\n",
    "\n",
    "print(f'F1 score: {calc_f1_score(model, test_data_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. GRU Encoder Model\n",
    "\n",
    "We'll repeat the last part with replacing LSTM with GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUSemanticRoleLabeler(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, labels_count):\n",
    "        super(GRUSemanticRoleLabeler, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, labels_count)\n",
    "\n",
    "    def forward(self, sentence, verb_indices):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        gru_out, _ = self.gru(embeds)\n",
    "        \n",
    "        verb_hidden_states = gru_out[torch.arange(gru_out.size(0)), verb_indices]\n",
    "        verb_hidden_states_expanded = verb_hidden_states.unsqueeze(1).expand(-1, gru_out.size(1), -1)\n",
    "        concatenated_states = torch.cat((gru_out, verb_hidden_states_expanded), dim=2)\n",
    "\n",
    "        label_space = self.hidden2label(concatenated_states)\n",
    "        label_scores = F.log_softmax(label_space, dim=1)\n",
    "        return label_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUSemanticRoleLabeler(EMBEDDING_DIM, HIDDEN_DIM, len(dataset.text_vocab), len(SRL_TO_NUM))\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracies = train_model(model, loss_function, optimizer, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'F1 score: {calc_f1_score(model, test_data_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Encoder-Decoder Model\n",
    "\n",
    "For this part we are going to generate a series of questions and answers. Then we'll feed these question and answers to an encoder-decoder model and use it to solve the SRL task. Let's first implement the functions to generate the question and answers from the datasets. generating questions would require extracting the predicate and adding the label to the end of the questions. This is the general format of a question: \\[Predicate\\] \\[SEPT\\] \\[Sentence\\] \\[Label\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPT_TOKEN = '[SEPT]'\n",
    "START_TOKEN = '<s>'\n",
    "END_TOKEN = '</s>'\n",
    "labels = {label[2:] for label in SRL_TO_NUM.keys() if len(label) > 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(sentence: list[str], verb_index: int) -> list[list[str]]:\n",
    "    return [\n",
    "        [sentence[verb_index]] + [SEPT_TOKEN] + sentence + [label] for label in labels\n",
    "    ]\n",
    "\n",
    "print(generate_questions(['I', 'ran', 'a', 'code', 'printing', 'hello', 'world'], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step would be generating answers for each question. we'll use the order in `labels` to generate answers for every question in a parallel list. Note that we have to parse the `B-label` and `I-Labels`, so we write the `parse_label` function for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_label(sentence: list[str], sentence_labels: list[str], label: str) -> list[str]:\n",
    "    simplified_labels = [label[2:] for label in sentence_labels]\n",
    "\n",
    "    if label not in simplified_labels:\n",
    "        return []\n",
    "\n",
    "    label_start_index = simplified_labels.index(label)\n",
    "    \n",
    "    if label not in simplified_labels[label_start_index + 1:]:\n",
    "        return [sentence[label_start_index]]\n",
    "\n",
    "    label_end_index = simplified_labels.index(label, label_start_index + 1)\n",
    "    return sentence[label_start_index:label_end_index + 1]\n",
    "\n",
    "\n",
    "test_sentence = [\"In\", \"the\", \"summer\", \"of\", \"2005\", \",\", \"a\", \"picture\", \"that\", \"people\", \"have\", \"long\", \"been\", \"looking\",\n",
    "                 \"forward\", \"to\", \"started\", \"emerging\", \"with\", \"frequency\", \"in\", \"various\", \"major\", \"Hong\", \"Kong\", \"media\", \".\"]\n",
    "test_label = [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ARG0\", \"O\", \"B-ARGM-TMP\",\n",
    "               \"O\", \"O\", \"B-ARG1\", \"I-ARG1\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "\n",
    "print(parse_label(test_sentence, test_label, 'ARG1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(sentence: list[str], sentence_labels: list[str]) -> list[list[str]]:\n",
    "    return [\n",
    "        [START_TOKEN] + parse_label(sentence, sentence_labels, label) + [END_TOKEN] for label in labels\n",
    "    ]\n",
    "\n",
    "print(generate_answers(test_sentence, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have functions to generate questions and answers, let's implement a dataset object that will return a question and an answer while being encoded as a tensor of their indexes. We'll do this with the help of `Vocab` class written in the part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Vocab):\n",
    "        padded_sentences = pad_data_sequence(df['text'].tolist(), Vocab.PAD)\n",
    "        padded_labels = pad_data_sequence(df['srl_frames'].tolist(), 'O')\n",
    "\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for index, row in df.iterrows():\n",
    "            sentence = padded_sentences[index]\n",
    "            sentence_labels = padded_labels[index]\n",
    "            verb_index = row['verb_index']\n",
    "\n",
    "            questions.extend(generate_questions(sentence, verb_index))\n",
    "            answers.extend(generate_answers(sentence, sentence_labels))\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        QADataset._add_special_tokens_to_vocab(self.vocab)\n",
    "\n",
    "        self.questions = self.vocab.to_input_tensor(questions)\n",
    "        self.answers = self.vocab.to_input_tensor(answers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.questions[index]), torch.tensor(self.answers[index])\n",
    "    \n",
    "    def _add_special_tokens_to_vocab(vocab: Vocab) -> None:\n",
    "        vocab.add(START_TOKEN)\n",
    "        vocab.add(END_TOKEN)\n",
    "        vocab.add(SEPT_TOKEN)\n",
    "\n",
    "        for label in labels:\n",
    "            vocab.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vocab = Vocab.from_corpus(train_data['text'], VOCAB_SIZE, 0, 0)\n",
    "\n",
    "dataset = QADataset(train_data, train_text_vocab)\n",
    "data_loader = data.DataLoader(dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look into out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_batch, answers_batch = next(iter(data_loader))\n",
    "\n",
    "print(f'First question: {dataset.vocab.indices2words([questions_batch[0].tolist()])}')\n",
    "print(f'First answer: {dataset.vocab.indices2words([answers_batch[0].tolist()])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the GloVe vectors and create the embedding matrix so that we can use it in our embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "\n",
    "glove_embeddings = {}\n",
    "with open('./glove/glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        line.strip()\n",
    "        line_tokens = line.split()\n",
    "        glove_embeddings[line_tokens[0]] = np.asarray(line_tokens[1:], 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights_matrix = np.zeros((len(dataset.vocab), EMBEDDING_DIM))\n",
    "words_found = 0\n",
    "\n",
    "for word, index in dataset.vocab.word2id.items():\n",
    "    try:\n",
    "        embedding_weights_matrix[index] = glove_embeddings[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        embedding_weights_matrix[index] = np.random.normal(scale=0.6, size=(EMBEDDING_DIM, ))\n",
    "\n",
    "print(f'{words_found} words found out of {len(dataset.vocab)} words in vocab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the encoder part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, question):\n",
    "        embeds = self.word_embeddings(question)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embeds)\n",
    "\n",
    "        return lstm_out, hidden, cell\n",
    "\n",
    "    def init_embeddings(self, pre_trained_embeddings: np.ndarray):\n",
    "        self.word_embeddings.load_state_dict({'weight': pre_trained_embeddings})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the decoder with attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoderWithAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, labels_count):\n",
    "        super(LSTMDecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, labels_count)\n",
    "\n",
    "    def forward(self, seq_input, encoder_hidden_state):\n",
    "        lstm_out, (hidden, cell) = self.lstm(seq_input, encoder_hidden_state)\n",
    "        attn_out = self.attention(lstm_out, hidden)\n",
    "        self.linear(attn_out.squeeze(0))\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def attention(self, lstm_out, final_state):\n",
    "        hidden = final_state.squeeze(0)\n",
    "        attn_weights = torch.bmm(lstm_out, hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        new_hidden_state = torch.bmm(\n",
    "            lstm_out.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        \n",
    "        return new_hidden_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
