{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - CA1\n",
    "**Saman Eslami Nazari - 810199375**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "  pattern = r'\\b\\w+\\b'\n",
    "  tokens = re.findall(pattern, text)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. What kind of tokenizer does the code above indicate? Is it character-based, subword-based, or word-based?\n",
    "\n",
    "Let's first take a look into each kind of tokenization:\n",
    "  - Character-based: We split the input by each character resulting resulting an array of characters.\n",
    "  - Subword-based: We split the input by each word; After getting the list of words, we won't touch the commonly used words but we will break down the rare ones to more common and meaningful words, e.g. \"Tokenization\" to \"Token\" and \"ization\".\n",
    "  - Word-based: We split the input by words without changing the outcome (like we did in Subword-based).\n",
    "\n",
    "The code above will break the input word by word, which means that **it's word-based**. The `\\b` indicates delimiters between words. The `\\w` indicates word characters, therefore `\\w+` matches a single word.\n",
    "<br>\n",
    "*Problems*: There are some problems with the word-based technique:\n",
    "  - One problem with this approach is that for words with punctuations it will lead to incomprehensible tokens. Consider the word \"Don't\". This leads to tokens \"Don\" and \"t\", while a better tokenization could be \"Do\" and \"n't\".\n",
    "  - Also this technique doesn't know the similarity of the words with the same root. For example it doesn't the similarity between \"Apple\" and \"Apples\".\n",
    "  - Another problem with word-based tokenization is that it leads to huge vocabulary. On important reason for this issue is what was mentioned previously: Lack of understanding the similarities. With this technique we end up storing the same root word with different looks so many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Run the tokenization code below and show two problems with this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_part2_text = \"Just received my M.Sc. diploma today, on 2024/02/10! Excited to embark on this new journey of knowledge and discovery. #MScGraduate #EducationMatters.\"\n",
    "\n",
    "print(custom_tokenizer(q1_part2_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First big problem is that it separates \"M.Sc.\" into different parts. This word should not be split because it has meaning as whole. Also the token \"M\" can mean anything in different sentence which can mislead the model.\n",
    "- Second problem can be the date tokenization, which could be more meaningful as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Improve the `custom_tokenizer` to fix at least one of the problems mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def improved_custom_tokenizer(text: str):\n",
    "  tokens = text.split()\n",
    "  tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "  return tokens\n",
    "\n",
    "print(improved_custom_tokenizer(q1_part2_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach the input text will be split into words and then have their punctuations removed. So in this case \"M.Sc\" won't be split into two tokens and the date will remain in one piece. But this approach still doesn't solve the problem with root words. This problem requires more advances methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. What kind of tokenizer do BERT and GPT use? What's the reason?\n",
    "\n",
    "- BERT tokenizer uses subword-based method. It uses WordPiece to generate the vocabulary. The WordPiece technique generates subwords based on the likelihood of characters occurring together.\n",
    "- GPT also uses subword-based method. To make it work, it uses techniques like Byte Pair Encoding or SentencePiece.\n",
    "\n",
    "One reason for this decision is that it helps to handle the out-of-vocabulary tokens. It also helps us to handle larger variety of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Describe the two algorithms used by BERT and GPT models.\n",
    "\n",
    "#### Bert Method\n",
    "\n",
    "Bert uses WordPiece algorithm. It starts with a small special vocabulary from the model and the alphabet. It then starts to learn merge rules. The algorithm will calculate a score for each possible pair and then merges the one with the highest score. The formula for this score is like this:\n",
    "\n",
    "$$score = \\frac{frequency of pair}{frequency of first element \\times frequency of second element}$$\n",
    "\n",
    "With this function, the algorithm will prioritize the pairs that its individual parts are less frequent, meaning that frequent parts of words will not create new tokens in the vocabulary. For example \"dis\" and \"able\" might not be good candidates to merge because \"able\" is a frequent token and appears in many other words. But on the other hand \"hel\" and \"llo\" are more likely to be merged because they're both less frequent.\n",
    "\n",
    "#### GPT Method\n",
    "\n",
    "GPT uses Byte-Pair Encoding (BPE) Method. This algorithms starts with calculating the unique set of words available. Then it builds the vocabulary with the symbols used to make these words. After this it will start to learn merges. For every consecutive pair it will merge the most frequent ones. The result will be added to the vocabulary. This continues and the subwords grow bigger.\n",
    "\n",
    "#### Differences\n",
    "\n",
    "The main difference between these two algorithms is the score function that they use. The WordPiece considers frequency of elements as well while the BPE method only cares about the frequency of the pair itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Implement a simple version of these algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = open(\"./data/All_Around_the_Moon.txt\", encoding=\"utf8\")\n",
    "corpus = corpus_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "\n",
    "First We'll start by splitting the text into words and create the frequency map of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "words = re.findall(r\"[\\w']+|[.,!?;]\", corpus)\n",
    "words_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to create the initial vocabulary, which is the alphabet used in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {c for word in words for c in word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to count the occurrence of each pair and commit the merges, we need to calculate a map from each word to its characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word : [c for c in word] for word in words_freq.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we write the function to calculate the score of each pair. This function will be used in each step of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def word_piece_pairs_score(splits: dict[str, list], words_freq: Counter) -> dict[tuple[str, str], int]:\n",
    "  \"\"\"\n",
    "  This method calculate the score of each pair available in the corpus. It first\n",
    "  iterates over all the words in the corpus and count the frequency of each\n",
    "  symbol. It then count the frequency of each pair occurred in the words. At\n",
    "  last it calculates the score of each pair by the formula below:\n",
    "  current pair score = frequency of pair / (frequency of the first element * frequency of the second element)\n",
    "  \"\"\"\n",
    "\n",
    "  pair_elem_freq = defaultdict(int)\n",
    "  pair_freq = defaultdict(int)\n",
    "\n",
    "  for word, split in splits.items():\n",
    "    for pair_elem in split:\n",
    "      pair_elem_freq[pair_elem] += words_freq[word]\n",
    "    \n",
    "    if len(split) <= 1:\n",
    "      continue\n",
    "\n",
    "    for i in range(len(split) - 1):\n",
    "      pair_freq[(split[i], split[i + 1])] += words_freq[word]\n",
    "    \n",
    "  result_scores = defaultdict()\n",
    "  for pair, freq in pair_freq.items():\n",
    "    result_scores[pair] = freq / (pair_elem_freq[pair[0]] * pair_elem_freq[pair[1]])\n",
    "  \n",
    "  return result_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the scores, we need a method to merge the best possible pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(splits: dict[str, list], pair_to_merge: tuple[str, str]) -> None:\n",
    "  \"\"\"\n",
    "  Given a dictionary of splitted words and the designated pair, this function\n",
    "  will merge every pair in each word.\n",
    "  \"\"\"\n",
    "\n",
    "  re_pattern = f\"\\\\w*{pair_to_merge[0]}{pair_to_merge[1]}\\\\w*\"\n",
    "\n",
    "  for word, split in splits.items():\n",
    "    if not re.search(re_pattern, word): continue\n",
    "\n",
    "    for i in range(len(split) - 1):\n",
    "      if split[i] == pair_to_merge[0] and split[i + 1] == pair_to_merge[1]:\n",
    "        splits[word] = split[:i] + [pair_to_merge[0] + pair_to_merge[1]] + split[i + 2:]\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for the final function. It will get the words and expand the vocabulary until a certain size is reached. We will also repeat the first steps for calculating `split_words` and other variables to make the function self containing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(corpus: str, vocab_size: int) -> tuple[list[str], dict[tuple, str]]:\n",
    "  words = re.findall(r\"[\\w']+|[.,!?;]\", corpus)\n",
    "  words_freq = Counter(words)\n",
    "  vocab = {c for word in words for c in word}\n",
    "  splits = {word : [c for c in word] for word in words_freq.keys()}\n",
    "\n",
    "  merges = {}\n",
    "\n",
    "  while len(vocab) < vocab_size:\n",
    "    scores = word_piece_pairs_score(splits, words_freq)\n",
    "    best_pair = max(scores, key=scores.get)\n",
    "    merge_pair(splits, best_pair)\n",
    "    vocab.add(best_pair[0] + best_pair[1])\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "  \n",
    "  return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_piece_vocab, word_piece_merges = train_word_piece(corpus, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-Pair Encoding\n",
    "\n",
    "BPE and WordPiece are very similar in terms of algorithm structure. The only thing we need to do is to replace the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_pairs_score(splits: dict[str, list], words_freq: Counter) -> dict[tuple[str, str], int]:\n",
    "  \"\"\"\n",
    "  Scoring function for BPE algorithm. The score of each pair is simply the\n",
    "  frequency of that pair in the corpus.\n",
    "  \"\"\"\n",
    "\n",
    "  pair_freq = defaultdict(int)\n",
    "\n",
    "  for word, split in splits.items():\n",
    "    for i in range(len(split) - 1):\n",
    "      pair_freq[(split[i], split[i + 1])] += words_freq[word]\n",
    "  \n",
    "  return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(corpus: str, vocab_size: int) -> tuple[list[str], dict[tuple, str]]:\n",
    "  words = re.findall(r\"[\\w']+|[.,!?;]\", corpus)\n",
    "  words_freq = Counter(words)\n",
    "  vocab = {c for word in words for c in word}\n",
    "  splits = {word : [c for c in word] for word in words_freq.keys()}\n",
    "\n",
    "  merges = {}\n",
    "\n",
    "  while len(vocab) < vocab_size:\n",
    "    scores = bpe_pairs_score(splits, words_freq)\n",
    "    best_pair = max(scores, key=scores.get)\n",
    "    merge_pair(splits, best_pair)\n",
    "    vocab.add(best_pair[0] + best_pair[1])\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "  \n",
    "  return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocab, bpe_merges = train_bpe(corpus, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the size of the words in each method's vocabulary? Are these numbers different? Why?\n",
    "\n",
    "For this question I'll check the largest word created in each vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_larges_word = max(word_piece_vocab, key=len)\n",
    "print(f\"WordPiece largest word: {bpe_larges_word}, size = {len(bpe_larges_word)}\")\n",
    "\n",
    "word_piece_largest_word = max(bpe_vocab, key=len)\n",
    "print(f\"BPE larges word: {word_piece_largest_word}, size = {len(word_piece_largest_word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the largest word that the BPE created is 10 characters long while for the WordPiece is 13. The main reason behind this difference is the way they each calculate their pair scores. WordPiece give each pair a relational score, which means that with the growth of the size of the pairs the score doesn't reduce a lot in comparison to pairs with smaller size. But in BPE's case, as the pairs get merged, the frequency of bigger pairs reduce in comparison to smaller pairs, because it's more rare to see a unique word than to see a smaller sub-word. Therefore in BPE's case, in each iteration the smaller merges are more probable to choose so it takes longer to build larger words.<br>\n",
    "For example consider the pairs (\"e\", \"t\") and (\"pu\", \"sh\"). The number of the words that has the first pair are probably more than the number of the words that contain \"push\". But in the case of WordPiece we'll also take the frequency of the elements of the pair into account. We might have less \"push\"s than \"et\"s but we also have less \"pu\" and \"sh\"s than \"e\" and \"t\"s in our corpus, therefore the fraction for the second pair isn't *that* small in comparison to the first pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the given text.\n",
    "\n",
    "In order to tokenize a text we need a method to get the learned merge rules and apply it to the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(merge_rules: dict[tuple[str, str], str], text: str) -> list[str]:\n",
    "  \"\"\"\n",
    "  This method will first pre-tokenize the `text` and split it into characters.\n",
    "  After this it will try to merge the tokens using the provided rules.\n",
    "  \"\"\"\n",
    "\n",
    "  def merge_word(split_word: list[str], merge_rule: tuple[str, str, str]) -> list[str]:\n",
    "    i = 0\n",
    "    while i < (len(split_word) - 1):\n",
    "      if split_word[i] == merge_rule[0] and split_word[i + 1] == merge_rule[1]:\n",
    "        split_word = split_word[:i] + [merge_rule[2]] + split_word[i + 2:]\n",
    "      else:\n",
    "        i += 1\n",
    "    return split_word\n",
    "\n",
    "  split_text = [[c for c in word] for word in text.split()]\n",
    "  for pair, merged in merge_rules.items():\n",
    "    for idx, split_word in enumerate(split_text):\n",
    "      split_text[idx] = merge_word(split_word, (pair[0], pair[1], merged))\n",
    "    \n",
    "  return sum(split_text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_tokenize = \"\"\"\n",
    "This darkness is absolutely killing! If we ever take this trip again, it must be\n",
    "about the time of the sNew Moon! This is a tokenization task. Tokenization is\n",
    "the first step in a NLP pipeline. We will be comparing the tokens generated by\n",
    "each tokenization model.\n",
    "\"\"\"\n",
    "\n",
    "text_to_tokenize = text_to_tokenize.replace('\\n', ' ')\n",
    "\n",
    "tokenized_with_bpe = tokenize(bpe_merges, text_to_tokenize)\n",
    "tokenized_with_word_piece = tokenize(word_piece_merges, text_to_tokenize)\n",
    "\n",
    "print(f\"BPE: {tokenized_with_bpe}\")\n",
    "print(f\"WordPiece: {tokenized_with_word_piece}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = open(\"./data/Tarzan.txt\", encoding=\"utf8\")\n",
    "corpus = corpus_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Pre-process your data and train your tokenizer.\n",
    "\n",
    "For this purpose I'll use the hugging face WordPiece Tokenizer. The first thing to do is to normalize the data using the `BertNormalizer` in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we'll normalize the data using `NFD`, `LowerCase`, and `StripAccent` normalizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pre-tokenize the data. We can use the `BertPreTokenizer` to split the text based on whitespace and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed our tokenization pipeline, we'll have to train it. We have to create a `WordPieceTrainer` and use it to train out tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"./data/Tarzan.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step for out tokenizer would be adding a post_processor so that it adds special tokens to the start and end of each sentence. We'll use `TemplateProcessor` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tokenizer is ready to encode text inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Train a bi-gram model from the corpus. Also, what's data sparsity and how will you handle it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sparsity happens when there is missing data in the corpus. In the n-grams' case we might learn many different combinations of words from our corpus but still we might encounter new combinations in the test data. In this case we estimate the probability of that combination to zero, which is not true. In order to solve this problem, there are many solutions such as add-1 smoothing, backoff, or interpolation. In this project we'll be using backoff method to solve the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step to create a bi-gram model is to create a method that will generate n-grams from tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(text: str, n: int, tokenizer: Tokenizer) -> list[tuple[str]]:\n",
    "  \"\"\"\n",
    "  This method will first tokenize the `text` using the provided `tokenizer`.\n",
    "  After doing that it will create n-grams with respect to the given `n`.\n",
    "  \"\"\"\n",
    "\n",
    "  tokens = tokenizer.encode(text).tokens\n",
    "  result_n_grams = []\n",
    "  idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n) \n",
    "  for i in idx_range:\n",
    "    result_n_grams = result_n_grams + [tuple(tokens[i:i+n])]\n",
    "  return result_n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can create n-grams, let's train our model. Our n-gram model is simply the probability of seeing a word after another:\n",
    "$$p(w_i|w^{i-1}_{i-k+1})=\\frac{count(w^i_{i-k+1})}{count(w^{i-1}_{i-k+1})}$$\n",
    "We'll write the function that will calculate this probability for each gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_n_gram(text: str, n: int, tokenizer: Tokenizer) -> dict[tuple[str], int]:\n",
    "  \"\"\"\n",
    "  This method calculate the probability of seeing the nth word after seeing\n",
    "  (n-1) words before it. To do it counts the number of times we've seen the\n",
    "  sentence with n words (`big_sentence_count`) and the number of times it's seen\n",
    "  the sentence with (n-1) words (`small_sentence_count`). the result will be =\n",
    "  `big_sentence_count` \\ `small_sentence_count`.\n",
    "  \"\"\"\n",
    "\n",
    "  big_sentences = Counter(get_n_gram(text, n, tokenizer))\n",
    "  small_sentences = Counter(get_n_gram(text, n - 1, tokenizer))\n",
    "\n",
    "  result = {}\n",
    "  for big_sentence, big_sentence_count in big_sentences.items():\n",
    "    small_sentence_count = small_sentences[big_sentence[:-1]]\n",
    "    result[big_sentence] = big_sentence_count / small_sentence_count\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Predict the following sentences with at least 10 more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we are going to use backoff method to solve the data sparsity. Therefore We will need a method that will create n'-grams for n' from 1 to the designated n and use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_grams(text: str, n: int, tokenizer: Tokenizer) -> list[dict[tuple[str], int]]:\n",
    "  \"\"\"\n",
    "  This method will create n-grams for n from 1 to the designated `n`. Th result\n",
    "  will be a list of these trained n-grams where the index 0 of the list will\n",
    "  correspond to a uni-gram.\n",
    "  \"\"\"\n",
    "\n",
    "  result = [None] * n\n",
    "  for i in range(1, n + 1):\n",
    "    result[i - 1] = train_n_gram(text, i, tokenizer)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing that we need is a method to choose the next word with respect to the previous words and the trained n-gram. Note that the following implementation is not the best as it iterates over the dictionary's keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "def predict_next_word(previous_text: list[str], n_gram: dict[tuple[str], int]) -> str | None:\n",
    "    \"\"\"\n",
    "    This method simply searches for every combination of words in the n_gram\n",
    "    that matches the input text. After finding every matched combination, it\n",
    "    will make a random choice with the probabilities found in n_gram.\n",
    "    \"\"\"\n",
    "    matched_combs: list[tuple[str]] = []\n",
    "    combs_probabilities: list[int] = []\n",
    "    previous_text = tuple(previous_text)\n",
    "\n",
    "    for words_comb, probability in n_gram.items():\n",
    "       if previous_text == words_comb[:-1]:\n",
    "         matched_combs += [words_comb]\n",
    "         combs_probabilities += [probability]\n",
    "    \n",
    "    if not matched_combs:\n",
    "      return None\n",
    "\n",
    "    return choices(matched_combs, combs_probabilities)[0][-1] # Select the last word of the chosen n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function would be to predict the given text `n` times and backoff to lower n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(\n",
    "    init_sentence: str,\n",
    "    n_tokens: int,\n",
    "    n: int,\n",
    "    trained_n_grams: list[dict[list[str], int]],\n",
    "    tokenizer: Tokenizer) -> list[str]:\n",
    "  \"\"\"\n",
    "  This method will continue the given initial sentence until `n_tokens` using\n",
    "  the trained n-grams. it will also backoff to a lower n-gram when ever it\n",
    "  doesn't find the sequence in the initial n-gram.\n",
    "  \"\"\"\n",
    "\n",
    "  result = tokenizer.encode(init_sentence).tokens[:-1] # Tokenize and remove the end of sentence special token\n",
    "  for i in range(n_tokens):\n",
    "    next_token = None\n",
    "    current_n = n\n",
    "    while next_token is None:\n",
    "      next_token = predict_next_word(result[-(current_n - 1):], trained_n_grams[current_n - 1])\n",
    "      current_n -= 1\n",
    "    \n",
    "    result += [next_token]\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train out n-grams. In this case we'll use bi-grams as the strongest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams = train_n_grams(corpus, 2, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's predict the sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_sentence_1 = \"Knowing well the windings of the trail he\"\n",
    "print(predict_text(init_sentence_1, 10, 2, trained_n_grams, tokenizer))\n",
    "init_sentence_2 = \"For half a day he lolled on the huge back and\"\n",
    "print(predict_text(init_sentence_2, 10, 2, trained_n_grams, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Now do it with 3-grams and 5-grams!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams_3 = train_n_grams(corpus, 3, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_text(init_sentence_1, 10, 3, trained_n_grams_3, tokenizer))\n",
    "print(predict_text(init_sentence_2, 10, 3, trained_n_grams_3, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 5-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams_5 = train_n_grams(corpus, 5, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_text(init_sentence_1, 10, 5, trained_n_grams_5, tokenizer))\n",
    "print(predict_text(init_sentence_2, 10, 5, trained_n_grams_5, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Can you increase the `n` as much as you want in a n-gram model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No you can't. The first problem that occurs would be the increase of computation to train such a model. The second and more important problem would be the increase in data sparsity; As we are looking for larger combinations of words, the probability of seeing more larger combinations reduce dramatically leading the model to learn only a few combinations of words. For example, for the initial string of \"Knowing Well the windings\", there are less such combinations in the corpus to guess the fifth word from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the pre-written codes for training n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "data = pd.read_csv('./data/google_play_store_apps_reviews.csv')\n",
    "\n",
    "# Step 2: Split the data\n",
    "train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build the n-gram Language Model\n",
    "def get_ngrams(text: str, n: int) -> list[str]:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "def train_ngram(data: list, n: int) -> tuple[FreqDist, FreqDist]:\n",
    "    positive_ngrams = []\n",
    "    negative_ngrams = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        grams = get_ngrams(row['review'], n)\n",
    "        if row['polarity'] == 1:\n",
    "            positive_ngrams.extend(grams)\n",
    "        elif row['polarity'] == 0:\n",
    "            negative_ngrams.extend(grams)\n",
    "\n",
    "    positive_freq = FreqDist(positive_ngrams)\n",
    "    negative_freq = FreqDist(negative_ngrams)\n",
    "\n",
    "    return positive_freq, negative_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the Model\n",
    "n = 2  # Change to the desired n-gram size\n",
    "positive_freq, negative_freq = train_ngram(train_data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the trained data is ready, let's write a method that would predict the label of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(text: str, positive_freq: FreqDist, negative_freq: FreqDist, n: int) -> int:\n",
    "  \"\"\"\n",
    "  This method will simply sum up the number of times that a bi-gram has occurred\n",
    "  in each class, normalize the result and choose whichever that has bigger\n",
    "  number.\n",
    "  \"\"\"\n",
    "\n",
    "  positive_size = sum(positive_freq.values())\n",
    "  negative_size = sum(negative_freq.values())\n",
    "\n",
    "  positive_occurrence, negative_occurrence = 0, 0\n",
    "\n",
    "  text_n_grams = get_ngrams(text, n)\n",
    "  for n_gram in text_n_grams:\n",
    "    positive_occurrence += positive_freq[n_gram]\n",
    "    negative_occurrence += negative_freq[n_gram]\n",
    "  \n",
    "  positive_occurrence = positive_occurrence * negative_size / positive_size\n",
    "\n",
    "  if positive_occurrence > negative_occurrence: return 1\n",
    "  return 0\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to test!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: test the n-gram\n",
    "def test_ngram(data: list, positive_freq: FreqDist, negative_freq: FreqDist, n: int) -> list:\n",
    "  pred_labels = []\n",
    "\n",
    "  for text in data:\n",
    "    pred_labels.extend([predict_label(text, positive_freq, negative_freq, n)])\n",
    "\n",
    "  return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = test_ngram(test_data[\"review\"].values.tolist(), positive_freq, negative_freq, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy Score: {accuracy_score(test_data['polarity'].values.tolist(), predicted_labels)}\")\n",
    "print(f\"Precision: {precision_score(test_data['polarity'].values.tolist(), predicted_labels)}\")\n",
    "print(f\"Recall: {recall_score(test_data['polarity'].values.tolist(), predicted_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here wa have an accuracy of 74% which says that out of all of out predictions, how many of them were true.\n",
    "We also calculated precision and recall. Based on the results, 55% of the positive predictions that we made were actually positive (recall), and 68% of the actual positive classes were predicted right by the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
