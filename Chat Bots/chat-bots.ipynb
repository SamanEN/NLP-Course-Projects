{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9_U7IpVvyjD"
      },
      "source": [
        "# Chat Bots\n",
        "\n",
        "NLP - Spring Semester of 2024 at University of Tehran - CA6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T10:59:12.654168Z",
          "iopub.status.busy": "2024-06-25T10:59:12.653797Z",
          "iopub.status.idle": "2024-06-25T11:00:25.382619Z",
          "shell.execute_reply": "2024-06-25T11:00:25.381503Z",
          "shell.execute_reply.started": "2024-06-25T10:59:12.654134Z"
        },
        "id": "hgVYWoQIvyjG",
        "outputId": "5974bffa-8718-4c02-cb52-ecc05412ea1c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet requests\\\n",
        "    beautifulsoup4\n",
        "\n",
        "%pip install --upgrade --quiet langchain langchain-community langchain-together\\\n",
        "    unstructured[pdf]\\\n",
        "    langchain-huggingface\\\n",
        "    faiss-cpu faiss-gpu\\\n",
        "    rank_bm25\\\n",
        "    tavily-python\n",
        "\n",
        "%pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:25.385407Z",
          "iopub.status.busy": "2024-06-25T11:00:25.384990Z",
          "iopub.status.idle": "2024-06-25T11:00:26.350804Z",
          "shell.execute_reply": "2024-06-25T11:00:26.350038Z",
          "shell.execute_reply.started": "2024-06-25T11:00:25.385348Z"
        },
        "id": "DpPD-c_AvyjI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "import os.path\n",
        "from textwrap import dedent\n",
        "\n",
        "from langchain_community.document_loaders import OnlinePDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "from langchain_together import ChatTogether\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from typing import Literal\n",
        "\n",
        "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_core.documents.base import Document\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from typing import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages.base import BaseMessage\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "\n",
        "from IPython.core.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbmiyPEUvyjJ"
      },
      "source": [
        "## Part 1. Getting the Required Data\n",
        "\n",
        "In this project we are going to use the pdfs of \"Speech and language Processing\" by Dan Jurafsky and James H. Martin to build a chat bot that will answer your NLP questions! For this purpose we'll first download the html page of the book and scrape out the links to each chapter. We'll then download and store them all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:26.352934Z",
          "iopub.status.busy": "2024-06-25T11:00:26.352227Z",
          "iopub.status.idle": "2024-06-25T11:00:26.462484Z",
          "shell.execute_reply": "2024-06-25T11:00:26.461574Z",
          "shell.execute_reply.started": "2024-06-25T11:00:26.352900Z"
        },
        "id": "rNXv-RqcvyjJ",
        "outputId": "39e1a78f-f3a9-4cbf-c814-786ce5fc8421",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "url = 'https://stanford.edu/~jurafsky/slp3/'\n",
        "response = requests.get(url)\n",
        "html_content = None\n",
        "\n",
        "if response.status_code == 200:\n",
        "    html_content = response.content\n",
        "    print(\"Successfully fetched the web page!\")\n",
        "else:\n",
        "    print(\"Failed to fetch the web page!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:26.464862Z",
          "iopub.status.busy": "2024-06-25T11:00:26.464572Z",
          "iopub.status.idle": "2024-06-25T11:00:26.484411Z",
          "shell.execute_reply": "2024-06-25T11:00:26.483623Z",
          "shell.execute_reply.started": "2024-06-25T11:00:26.464838Z"
        },
        "id": "hJ4r_KA9vyjK",
        "outputId": "0cec7ce5-e2a3-41f0-f30f-38115722f819",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "links = soup.find_all('a', href=True)\n",
        "links = [link['href'] for link in links if re.search(r'^\\d+\\.pdf$', link['href'])]\n",
        "links = [urljoin(url, link) for link in links]\n",
        "\n",
        "print(links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qwEeQXtvyjK"
      },
      "source": [
        "Now that we have the links, we'll load them in a suitable format with LangChain tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:26.485740Z",
          "iopub.status.busy": "2024-06-25T11:00:26.485464Z",
          "iopub.status.idle": "2024-06-25T11:02:35.813821Z",
          "shell.execute_reply": "2024-06-25T11:02:35.813041Z",
          "shell.execute_reply.started": "2024-06-25T11:00:26.485716Z"
        },
        "id": "V-wCLCVgvyjK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for link in links:\n",
        "    loader = OnlinePDFLoader(link)\n",
        "    documents.extend(loader.load())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfCm8HAuvyjL"
      },
      "source": [
        "Split the text into smaller chunks using the `langchain-text-splitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:02:35.815278Z",
          "iopub.status.busy": "2024-06-25T11:02:35.814856Z",
          "iopub.status.idle": "2024-06-25T11:02:36.071857Z",
          "shell.execute_reply": "2024-06-25T11:02:36.070933Z",
          "shell.execute_reply.started": "2024-06-25T11:02:35.815252Z"
        },
        "id": "WgtYEh0ovyjL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=64,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "document_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZDmHBvvyjM"
      },
      "source": [
        "## Part 2. Embedding and Storing Vectors\n",
        "\n",
        "Now that we have our documents ready, let's vectorize them using hugging face embedding vectors. We'll then use Faiss to efficiently store and query these vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:03:23.821609Z",
          "iopub.status.busy": "2024-06-25T11:03:23.821024Z",
          "iopub.status.idle": "2024-06-25T11:03:25.113796Z",
          "shell.execute_reply": "2024-06-25T11:03:25.112926Z",
          "shell.execute_reply.started": "2024-06-25T11:03:23.821577Z"
        },
        "id": "KAmQ3xcIvyjM",
        "outputId": "59c6a2fd-e7f8-4553-eaeb-a414be06c352",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "underlying_embedding = HuggingFaceEmbeddings()\n",
        "store = LocalFileStore('./cache/')\n",
        "\n",
        "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "    underlying_embedding, store\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:03:35.691405Z",
          "iopub.status.busy": "2024-06-25T11:03:35.690532Z",
          "iopub.status.idle": "2024-06-25T11:04:02.027973Z",
          "shell.execute_reply": "2024-06-25T11:04:02.027084Z",
          "shell.execute_reply.started": "2024-06-25T11:03:35.691348Z"
        },
        "id": "L8ZY043PvyjN",
        "outputId": "f22a95a7-85b4-498a-a47b-92abfb9259f1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "faiss_vector_store = None\n",
        "\n",
        "embeddings_db_file = \"faiss_index\"\n",
        "\n",
        "if not os.path.isfile(embeddings_db_file):\n",
        "    faiss_vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
        "    faiss_vector_store.save_local(embeddings_db_file)\n",
        "else:\n",
        "    faiss_vector_store = FAISS.load_local(embeddings_db_file)\n",
        "\n",
        "print(faiss_vector_store.index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qsBhswgvyjN"
      },
      "source": [
        "## Part 3. Implementing Retrievers\n",
        "\n",
        "In this part we'll use the `EnsembleRetriever` to combine a semantic retriever with a lexical one. We'll use `FAISS` and `BM25Retriever` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:08.802317Z",
          "iopub.status.busy": "2024-06-25T11:09:08.801794Z",
          "iopub.status.idle": "2024-06-25T11:09:09.063316Z",
          "shell.execute_reply": "2024-06-25T11:09:09.062523Z",
          "shell.execute_reply.started": "2024-06-25T11:09:08.802285Z"
        },
        "id": "Ji0OhGvCvyjN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bm25_retriever = BM25Retriever.from_texts(\n",
        "    [doc.page_content for doc in document_chunks]\n",
        ")\n",
        "bm25_retriever.k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:14.696517Z",
          "iopub.status.busy": "2024-06-25T11:09:14.695896Z",
          "iopub.status.idle": "2024-06-25T11:09:14.700762Z",
          "shell.execute_reply": "2024-06-25T11:09:14.699767Z",
          "shell.execute_reply.started": "2024-06-25T11:09:14.696484Z"
        },
        "id": "OoM6pZUXvyjO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vector_store.as_retriever(\n",
        "    search_kwargs={'k': 2}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv6PNrlPvyjO"
      },
      "source": [
        "Test both retrievers before taking the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:52.774189Z",
          "iopub.status.busy": "2024-06-25T11:09:52.773314Z",
          "iopub.status.idle": "2024-06-25T11:09:52.798243Z",
          "shell.execute_reply": "2024-06-25T11:09:52.797329Z",
          "shell.execute_reply.started": "2024-06-25T11:09:52.774154Z"
        },
        "id": "RvtR8dKCvyjO",
        "outputId": "60b50341-ea13-4d89-8b26-2d2aafa7646c",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "related_query = \"Recurrent Neural Networks\"\n",
        "unrelated_query = \"Red-black trees\"\n",
        "super_unrelated_query = \"President of Congo\"\n",
        "\n",
        "faiss_retriever.invoke(related_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:10:03.255175Z",
          "iopub.status.busy": "2024-06-25T11:10:03.254440Z",
          "iopub.status.idle": "2024-06-25T11:10:03.267224Z",
          "shell.execute_reply": "2024-06-25T11:10:03.266360Z",
          "shell.execute_reply.started": "2024-06-25T11:10:03.255143Z"
        },
        "id": "_kY02eA0vyjP",
        "outputId": "8afede1c-2de7-405a-bde6-e3a5733ba5bd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bm25_retriever.invoke(related_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW7x_MW3vyjP"
      },
      "source": [
        "Now Let's ensemble these two!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:10:10.596895Z",
          "iopub.status.busy": "2024-06-25T11:10:10.596515Z",
          "iopub.status.idle": "2024-06-25T11:10:10.681739Z",
          "shell.execute_reply": "2024-06-25T11:10:10.680615Z",
          "shell.execute_reply.started": "2024-06-25T11:10:10.596864Z"
        },
        "id": "-f2xUk20vyjP",
        "outputId": "ac493649-f61f-4179-e2fa-8552caedbeeb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, faiss_retriever], wights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "print(ensemble_retriever.invoke(related_query))\n",
        "print(ensemble_retriever.invoke(unrelated_query))\n",
        "print(ensemble_retriever.invoke(super_unrelated_query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvrbojBjvyjP"
      },
      "source": [
        "## Part 4. Router Chain\n",
        "\n",
        "For the first part of this chat bot, we need to make a chain that will decide what class does the user question belong to. We'll start by making a chat template to guide the LLM through our routing task. It will ask the model to classify the given question as on of three kinds: `NLP`, `Computer Science`, `Other`.\n",
        "\n",
        "We will then pass the prompt to the Llama model, using the TogetherAI API. In the end we'll use the Pydantic parser to parse the Llama result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leie7mEtvyjP"
      },
      "outputs": [],
      "source": [
        "router_prompt_template = \\\n",
        "    \"\"\"\n",
        "    You must route user queries to one of three classes: VectorStore, SearchEngine, or None.\n",
        "    If the user query is about Natural Language Processing and Speech Processing, choose VectorStore.\n",
        "    If the query is something about computer science but it's not related to NLP, SearchEngine.\n",
        "    If it's nothing about NLP or Computer Science, choose None.\n",
        "    Output only the chosen class. Do not output anything more than that.\n",
        "    {output_instruction}\n",
        "    query: {query}\n",
        "    \"\"\"\n",
        "router_prompt_template = dedent(router_prompt_template)\n",
        "\n",
        "router_prompt = ChatPromptTemplate.from_template(\n",
        "    template=router_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbAr--P2vyjQ"
      },
      "outputs": [],
      "source": [
        "together_ai_api_key = \"542f31d103435d11e271b87c2e5d84454ad362059198d0a4851d62a8587adf80\"\n",
        "\n",
        "router_llm = ChatTogether(\n",
        "    together_api_key=together_ai_api_key,\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHETN97lvyjQ"
      },
      "outputs": [],
      "source": [
        "class QueryKind(BaseModel):\n",
        "    class_name: Literal[\"VectorStore\", \"SearchEngine\", \"None\"] = Field()\n",
        "\n",
        "router_parser = PydanticOutputParser(pydantic_object=QueryKind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMOEF1P3vyjQ",
        "outputId": "6a86546c-b33c-4892-fbeb-a2aead81988c"
      },
      "outputs": [],
      "source": [
        "router_chain = router_prompt | router_llm | router_parser\n",
        "\n",
        "nlp_test_result = router_chain.invoke({\n",
        "    \"query\": \"How should I implement an LSTM model?\",\n",
        "    \"output_instruction\": router_parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "cs_test_result = router_chain.invoke({\n",
        "    \"query\": \"What's a redd-black tree?\",\n",
        "    \"output_instruction\": router_parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "other_test_result = router_chain.invoke({\n",
        "    \"query\": \"Who's the president of Congo?\",\n",
        "    \"output_instruction\": router_parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(f\"NLP: {nlp_test_result}, CS: {cs_test_result}, Other: {other_test_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJqCM4J7vyjQ"
      },
      "source": [
        "## Part 5. Search Engine Chain\n",
        "\n",
        "We've implemented the vector store function, and the router chain. Now in order to answer user queries about computer science, outside NLP, we'll need to use a search engine to retrieve documents. We'll then use one of the two methods, local vector store or a remote search engine to provide our language model some context to answer the user query.\n",
        "We'll be using Tavily AI for our search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIaxAcosvyjQ",
        "outputId": "22b7fa93-d539-4398-c9da-d134f15f8ffa"
      },
      "outputs": [],
      "source": [
        "tavily_api_key = \"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\"\n",
        "\n",
        "tavily_search_wrapper = TavilySearchAPIWrapper(tavily_api_key=tavily_api_key)\n",
        "tavily_search = TavilySearchResults(api_wrapper=tavily_search_wrapper, max_results=5)\n",
        "\n",
        "tavily_search.invoke(\"What's NLP?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLS-qAotvyjQ"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def parse_search_engine(documents: list[dict[str, str]]) -> list[Document]:\n",
        "    result_documents = [Document(\n",
        "        page_content=doc['content'],\n",
        "        metadata={'url': doc['url']}\n",
        "    ) for doc in documents]\n",
        "\n",
        "    return result_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ4qdUcnvyjR",
        "outputId": "050ed87f-3ed9-41de-8d40-8d4be1fc4f76"
      },
      "outputs": [],
      "source": [
        "search_engine_chain = tavily_search | parse_search_engine\n",
        "\n",
        "search_engine_chain.invoke(\"What's NLP?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbi91kixvyjR"
      },
      "source": [
        "## Part 6. Relevancy Check Chain\n",
        "\n",
        "We need another chain that prompts an LLM and ask it if the retrieved documents are relevant to the question or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDF2hFR6vyjR"
      },
      "outputs": [],
      "source": [
        "relevancy_check_template = \\\n",
        "    \"\"\"\n",
        "    You are provided with q user question and a document. If the given document is relevant to the user question and can be used to answer it, output 'Relevant', and if not, output 'Irrelevant'. Only output the words Relevant and Irrelevant in a JSON format as described in the output instructions.\n",
        "    User question: {user_query}\n",
        "    Document: {retrieved_document}\n",
        "    Output instruction: {output_instruction}\n",
        "    \"\"\"\n",
        "relevancy_check_template = dedent(relevancy_check_template)\n",
        "\n",
        "relevancy_check_prompt = ChatPromptTemplate.from_template(\n",
        "    template=relevancy_check_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSCZU9lsvyjR"
      },
      "outputs": [],
      "source": [
        "relevancy_check_llm = ChatTogether(\n",
        "    together_api_key=together_ai_api_key,\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu83pg3YvyjS"
      },
      "outputs": [],
      "source": [
        "class RelevancyKind(BaseModel):\n",
        "    class_name: Literal[\"Relevant\", \"Irrelevant\"] = Field()\n",
        "\n",
        "relevancy_check_parser = PydanticOutputParser(pydantic_object=RelevancyKind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufv8ROMrvyjS",
        "outputId": "86836926-47d0-4af9-ebd2-c2705e3519c4"
      },
      "outputs": [],
      "source": [
        "relevancy_check = relevancy_check_prompt | relevancy_check_llm | relevancy_check_parser\n",
        "\n",
        "user_query = \"How to implement recurrent neural network?\"\n",
        "retrieved_document = search_engine_chain.invoke(user_query)[0]\n",
        "result = relevancy_check.invoke({\n",
        "    \"user_query\": user_query,\n",
        "    \"retrieved_document\": retrieved_document,\n",
        "    \"output_instruction\": relevancy_check_parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(f'Retrieved Document: {retrieved_document}')\n",
        "print(f'Relevancy Check: {result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONFyD_LVvyjS"
      },
      "source": [
        "Let's also trick the model and see if it succeeds or not!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKFf6j-GvyjS",
        "outputId": "9968575f-650c-4b12-e9d0-6cc88ece1b52"
      },
      "outputs": [],
      "source": [
        "trick_user_query = \"Who was the first person to land on the moon?\"\n",
        "relevancy_check.invoke({\n",
        "    \"user_query\": trick_user_query,\n",
        "    \"retrieved_document\": retrieved_document,\n",
        "    \"output_instruction\": relevancy_check_parser.get_format_instructions()\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7fWDfyXvyjS"
      },
      "source": [
        "## Part 7. Fallback Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvtYGrf_vyjS"
      },
      "outputs": [],
      "source": [
        "fallback_template = \\\n",
        "    \"\"\"\n",
        "    You are a friendly and kind teaching assistant. Your job is to provide educational material related to NLP and Speech Recognition to the human user. Do not respond to the queries that are outside the context of NLP and Speech Recognition. If a query is not related acknowledge your limitations.\n",
        "\n",
        "    Current conversation:\n",
        "\n",
        "    {chat_history}\n",
        "\n",
        "\n",
        "    Human: {query}\n",
        "    \"\"\"\n",
        "\n",
        "fallback_template = dedent(fallback_template)\n",
        "fallback_prompt = ChatPromptTemplate.from_template(fallback_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2DGLnPIvyjd"
      },
      "outputs": [],
      "source": [
        "def gather_chat_history(context) -> list[str]:\n",
        "    chat_history = []\n",
        "\n",
        "    for message in context['chat_history']:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            chat_history.append(f'Human: {message.content}')\n",
        "        else:\n",
        "            chat_history.append(f'AI: {message.content}')\n",
        "\n",
        "    return chat_history\n",
        "\n",
        "@chain\n",
        "def gather_info(context) -> dict:\n",
        "    return {\n",
        "        \"chat_history\": gather_chat_history(context),\n",
        "        \"query\": context['query']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkGFOAK3vyjd"
      },
      "outputs": [],
      "source": [
        "fallback_llm = ChatTogether(\n",
        "    together_api_key=together_ai_api_key,\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-sgW2XBvyjd"
      },
      "outputs": [],
      "source": [
        "fallback_chain = gather_info | fallback_prompt | fallback_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il1PW1Hrvyje"
      },
      "source": [
        "## Pat 8. Generate with Context Chain\n",
        "\n",
        "This chain will get the user query along with some retrieved documents, and try to answer the user with the help of an LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex8HLbBYvyje"
      },
      "outputs": [],
      "source": [
        "generate_template = \\\n",
        "    \"\"\"\n",
        "    You are a helpful assistant. Answer the query below based only on the provided context. If the given context is not relevant, DO NOT answer based on your own knowledge.\n",
        "\n",
        "    Context: {documents}\n",
        "\n",
        "    Query: {query}\n",
        "    \"\"\"\n",
        "\n",
        "generate_template = dedent(generate_template)\n",
        "generate_prompt = ChatPromptTemplate.from_template(generate_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp6xyQ_fvyje"
      },
      "outputs": [],
      "source": [
        "generate_llm = fallback_llm\n",
        "\n",
        "generate_with_context_chain = generate_prompt | generate_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NgZF7bvyje"
      },
      "source": [
        "## Part 9. Implementing the Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1kI_g4Rvyje"
      },
      "source": [
        "First we need a customized dictionary to save the chat history and other materials related to the conversation context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONUDhyd_vyje"
      },
      "outputs": [],
      "source": [
        "class BotState(TypedDict):\n",
        "    \"\"\"This is a class to save the current chat context and history with the bot.\"\"\"\n",
        "\n",
        "    query: str\n",
        "    chat_history: list[BaseMessage]\n",
        "    generation: str\n",
        "    documents: list[Document]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89yCuAldvyjf"
      },
      "source": [
        "Then we implement the nodes using the previously developed chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzdbFx4svyjf"
      },
      "outputs": [],
      "source": [
        "def router_node(state: dict) -> str:\n",
        "    try:\n",
        "        result = router_chain.invoke({\n",
        "            'query': state['query'],\n",
        "            'output_instruction': router_parser.get_format_instructions()\n",
        "        })\n",
        "\n",
        "        class_name = result.class_name\n",
        "    except:\n",
        "        return 'LLMFallback'\n",
        "\n",
        "    if class_name == 'None':\n",
        "        return 'LLMFallback'\n",
        "\n",
        "    return class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9pJ55j7vyjf"
      },
      "outputs": [],
      "source": [
        "def vector_store_node(state: dict) -> dict:\n",
        "    return {\n",
        "        \"documents\": ensemble_retriever.invoke(input=state['query'])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv8ffoZSvyjf"
      },
      "outputs": [],
      "source": [
        "def search_engine_node(state: dict) -> dict:\n",
        "    return {\n",
        "        \"documents\": search_engine_chain.invoke(state['query'])\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZmkBXVLvyjg"
      },
      "outputs": [],
      "source": [
        "def filter_docs_node(state: dict) -> dict:\n",
        "    documents = state['documents']\n",
        "    filtered_documents = []\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            relevancy = relevancy_check.invoke({\n",
        "                'user_query': state['query'],\n",
        "                'retrieved_document': doc,\n",
        "                'output_instruction': relevancy_check_parser.get_format_instructions()\n",
        "            })\n",
        "            relevancy = relevancy.class_name\n",
        "            if relevancy == 'Relevant':\n",
        "                filtered_documents.append(doc)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "    return {'documents': filtered_documents}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udDs1lIpvyjg"
      },
      "outputs": [],
      "source": [
        "def fallback_node(state: dict) -> dict:\n",
        "    return {\n",
        "        'generation': fallback_chain.invoke(state)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-aem6kvyjg"
      },
      "outputs": [],
      "source": [
        "def generate_with_context_node(state: dict) -> dict:\n",
        "    return {\n",
        "        'generation': generate_with_context_chain.invoke(state)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7SBZX7Svyjh"
      },
      "source": [
        "Now that all the nodes are ready and implemented, we can proceed to create the agent graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKFmjntXvyjh"
      },
      "outputs": [],
      "source": [
        "workflow = StateGraph(BotState)\n",
        "\n",
        "workflow.add_node('vector_store', vector_store_node)\n",
        "workflow.add_node('search_engine', search_engine_node)\n",
        "workflow.add_node('fallback', fallback_node)\n",
        "workflow.add_node('generate_with_context', generate_with_context_node)\n",
        "workflow.add_node('filter_docs', filter_docs_node)\n",
        "\n",
        "workflow.set_conditional_entry_point(\n",
        "    router_node,\n",
        "    {\n",
        "        'VectorStore': 'vector_store',\n",
        "        'SearchEngine': 'search_engine',\n",
        "        'LLMFallback': 'fallback'\n",
        "    }\n",
        ")\n",
        "workflow.add_edge('vector_store', 'filter_docs')\n",
        "workflow.add_edge('search_engine', 'filter_docs')\n",
        "workflow.add_conditional_edges(\n",
        "    'filter_docs',\n",
        "    lambda docs: 'search_engine' if len(docs) == 0 else 'generate_with_context',\n",
        "    {\n",
        "        'search_engine': 'search_engine',\n",
        "        'generate_with_context': 'generate_with_context'\n",
        "    }\n",
        ")\n",
        "workflow.add_edge('fallback', END)\n",
        "workflow.add_edge('generate_with_context', END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "VYmYya5nvyjh",
        "outputId": "56784d53-929e-4001-9e58-e66fbfe93bd5"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile(debug=False)\n",
        "display(\n",
        "    Image(\n",
        "        app.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API,\n",
        "        )\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhck5loCvyjh"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "eFrj38Gdvyjh",
        "outputId": "2eb11946-41ff-43c5-8856-e866fb434409"
      },
      "outputs": [],
      "source": [
        "response1 = app.invoke({'query': 'Hello! I wanna know about NLP.', 'chat_history': []})\n",
        "response1['chat_history'] = [HumanMessage(response1['query']), AIMessage(response1['generation'])]\n",
        "\n",
        "Markdown(response1['generation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "-XfNp07K0PQi",
        "outputId": "1c8bb01e-2519-4758-d931-adbcdc2600ae"
      },
      "outputs": [],
      "source": [
        "state = response1\n",
        "state['query'] = 'Teach me about RNN!'\n",
        "response2 = app.invoke(state)\n",
        "\n",
        "Markdown(response2['generation'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
