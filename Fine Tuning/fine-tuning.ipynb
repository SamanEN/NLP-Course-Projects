{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whNpxfbmKNEZ"
      },
      "source": [
        "# Fine Tuning\n",
        "\n",
        "NLP - Spring Semester of 2024 at University of Tehran - CA4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1fH7sHzJhwk",
        "outputId": "a46722aa-29d1-43cf-a706-19ce044c7d78"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install tokenizers\n",
        "!pip install torch\n",
        "!pip install evaluate\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3wmMWERKK-3"
      },
      "outputs": [],
      "source": [
        "import datasets as hf_datasets\n",
        "import evaluate\n",
        "\n",
        "import transformers\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from transformers import RobertaConfig\n",
        "from transformers import LlamaTokenizer\n",
        "from transformers import LlamaForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "from peft import get_peft_model\n",
        "from peft import LoraConfig, PromptEncoderConfig, TaskType\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "import torch\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku5SfFK-EoUD"
      },
      "source": [
        "## Q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRgKnL0ILZmC"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "For this project we'll use the nyu-mll/multi-nli dataset. We'll use the `datasets` from `huggingface` library. Here's a summary about this dataset from the [huggingface website](https://huggingface.co/datasets/nyu-mll/multi_nli):\n",
        "\n",
        ">The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUIokapqLgfE",
        "outputId": "2ad8bfda-d2c0-4fbb-e134-1f703c4eb5d4"
      },
      "outputs": [],
      "source": [
        "DATASET_DUMP_PATH = './dump'\n",
        "\n",
        "try:\n",
        "    print(\"Loading dataset from disk.\")\n",
        "    dataset = hf_datasets.DatasetDict.load_from_disk(DATASET_DUMP_PATH)\n",
        "except:\n",
        "    print(\"Dump file not found. Fallback to remote option.\")\n",
        "    dataset = hf_datasets.load_dataset(\"nyu-mll/multi_nli\")\n",
        "    dataset.save_to_disk(DATASET_DUMP_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Dqpf8WYY_n"
      },
      "source": [
        "Each slice of `multi-nli` has several columns:\n",
        "\n",
        "- `premise`, and `hypothesis`: Two sentences following eachother.\n",
        "- `premise`, and `hypothesis` parse: Each sentence as parsed by the Stanford PCFG Parser 3.5.2\n",
        "- `premise`, and `hypothesis` binary parse: Each sentence parsed in unlabeled binary-branching format\n",
        "- `genre`: A string feature showing 5 different features: {telephone, governemnt, travel, fiction, slate}\n",
        "- `label`: A classification label, 0 means entailment, 1 means neutral, and 2 means contradiction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUO-_H_wThdV",
        "outputId": "5225d725-3539-403e-9f72-711d8af43078"
      },
      "outputs": [],
      "source": [
        "print(f\"A row with entailing sentences: {random.choice(dataset['train'].filter(lambda x: x['label'] == 0))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftk9vQxBZOz_"
      },
      "source": [
        "### Part 1. Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPjDvPgDIzfH"
      },
      "source": [
        "#### Explain general fine-tuning method. Explain LoRA briefly.\n",
        "\n",
        "There are several ways to fine tune models. Traditionaly, there are two ways of doing this:<br>\n",
        "The first approach is to freeze the model and re-train only a subset of it. In this way we can preserve all that model has learnt from previous, probaably by feeding a huge amount of data to it, and also adapt the final layers to the specific task that we are looking forward to. This is cost effective approach because we don't need much computation power or data to train that subset of the model.<br>\n",
        "The second approach is to re-train the whole model with the newer, domain specific data. This requires a larger amount of data and also more computation power and time. According to [this article](https://www.turing.com/resources/finetuning-large-language-models), this approach is particularly beneficial when the task-specific dataset is large and significantly different from the pre-training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6bPWv4FwwuH"
      },
      "source": [
        "#### How Does LoRA work?\n",
        "\n",
        "To answer this question we'll refer to [this explanatory article](https://medium.com/@AIBites/lora-low-rank-adaptation-of-llms-paper-explained-5ae866871c8a). LoRA tries to make fine-tuning faster and more efficient by leveraging the idea of rank decomposition. The rank of a matrice shows how many of its rows or columns are linearly independent from eachother. If two rows are linearly dependant, we can easily show them as one row and a multiplier. This is the idea of rank decomposition; To decompose the linearly dependent rows of a matrix into smaller matrices. In LoRA's article they empirically show that the pre-trained weight matrices are low-ranked. Based on this hypothesis, they also assume that the updated weights after fine-tuning are also low-ranked. So they do the rank decomposition on these matrices and make the training process more efficiently, since we will work with smaller matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPMHZ0_dI90a"
      },
      "source": [
        "#### Explain prompt tuning and the difference between hard and soft tuning.\n",
        "\n",
        "Instead of training the LLM from the starting point, we can give it some prompt, specifically crafted for our target task, to guide the model. This means adding additional input tokens to the original user-given input, so that the model can better understand what the task is. We have two methods of doing so: hard and soft.\n",
        "\n",
        "- **Hard Prompting**: In this approach we manually craft a template, and put the inputs in that template. The template consists of several tokens (Probably written in natural language) that will guide the model for our task. This can be hard as finding the best tokens manually can be hard.\n",
        "\n",
        "- **Soft Prompting**: With this method, we freeze the model and instead train the freshly added token parameters. These parameters are supposed to add tokens to the input so that the prompt that we give to the model will guide it through our task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txeRUuN8mLfv"
      },
      "source": [
        "### Part 2. Training the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWNsvsYOcT6"
      },
      "source": [
        "#### Training the Model from Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7IOY1OYH6lM"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-4\n",
        "EPOCH_COUNT = 10\n",
        "\n",
        "MODEL_NAME = 'roberta-base'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQZ8EzsuGG60"
      },
      "source": [
        "For this part we are going to use the RoBERTa model from hugging face transformers library. Let's first create our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqVoQsa1GTbv",
        "outputId": "2bbf2b3d-d440-436e-e15a-eee6861f649e"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "test_text_to_tokenize = random.choice(dataset['train']['premise'])\n",
        "print(f\"Original text: {test_text_to_tokenize}, \\nTokenized: {tokenizer.tokenize(test_text_to_tokenize)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx1PSlKvLMKM"
      },
      "source": [
        "For the next step we'll need to create a preprocess function that concatenates `premise` and `hypothesis` and apply tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RKUchNotJR0"
      },
      "outputs": [],
      "source": [
        "def  preprocess_dataset(dataset: hf_datasets.Dataset, tokenizer: RobertaConfig):\n",
        "    return tokenizer(dataset['premise'], dataset['hypothesis'], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aysKAc0KtJR1"
      },
      "source": [
        "Apply it to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fS_jB0CtJR1",
        "outputId": "ac227eeb-4457-4fdf-aa0a-3d697895a3f8"
      },
      "outputs": [],
      "source": [
        "encoded_dataset = dataset.map(lambda x: preprocess_dataset(x, tokenizer), batched=True)\n",
        "print(encoded_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhphjG5ztJR1"
      },
      "source": [
        "We will use the `RobertaForSequenceClassification` model and train it from start. The NLI task is actually a classification task in which we give the two sentences to the model and ask it to predict the label. Note that our dataset class actually concatenate the premise and hypothesis sentence and give as input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQAT_utgtJR1",
        "outputId": "f21f55f2-23b5-4a80-fbfb-311ddb08c4fa"
      },
      "outputs": [],
      "source": [
        "model_config = RobertaConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_position_embeddings=512,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "model = RobertaForSequenceClassification(config=model_config)\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxvvEuJ6tJR2"
      },
      "source": [
        "The model we'll have 12 attention heads and 6 hidden layers. Note that the original `roberta-base` has 12 hidden layers. We'll use the `Trainer` from hugging face, in which it uses `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zqix274tJR3"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    'RoBerta-trained-from-start',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='no',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCH_COUNT,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIkOH-UfLNKR"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXxhbAlItJR3"
      },
      "source": [
        "Before training, due to shortage of computation resources we'll reduce the size of the dataset to 10% of its initial size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMVarGR6tJR3"
      },
      "outputs": [],
      "source": [
        "num_train_samples = int(len(encoded_dataset['train']) * 0.1)\n",
        "num_val_samples = int(len(encoded_dataset['validation_matched']) * 0.1)\n",
        "small_train_dataset = encoded_dataset['train'].shuffle(seed=42).select(range(num_train_samples))\n",
        "small_val_dataset = encoded_dataset['validation_matched'].shuffle(seed=42).select(range(num_val_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hy-fVGMJtJR4"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "hYzYhXk9tJR4",
        "outputId": "9e7f1483-6a00-40e7-9f30-c5589486824b"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(f'Elapsed time: {time.time() - start_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh7eBkSrB0hG"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz5NrboKOcT9"
      },
      "source": [
        "Here we can see a full description of accuracy and loss at the end of each epoch. We trained the model for 30 epochs with the learning rate equal to 2e-5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAGHJXY8OcT9"
      },
      "source": [
        "#### Using LoRA Fine Tuning to Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voiM1AobOcT-"
      },
      "outputs": [],
      "source": [
        "LORA_ALPHA = 32\n",
        "LORA_R = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MaVUvg8OcT-"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    r=LORA_R\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzBwvv4iOcT-",
        "outputId": "e416342b-d53b-4fcf-a22d-22a9538320ba"
      },
      "outputs": [],
      "source": [
        "model = RobertaForSequenceClassification(model_config)\n",
        "model = get_peft_model(model, peft_config)\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuxURctKOcT-"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    'RoBerta-LoRA-Fine-Tuned',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='no',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCH_COUNT,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM3qGkm-OcT-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "-3FsGeZ1OcT-",
        "outputId": "e885c1d1-344a-4e5c-f7ca-d743383abb65"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(f'Elapsed time: {time.time() - start_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxAyefOwB0hL"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeRmCpiDB0hM"
      },
      "source": [
        "#### Using Prompt Tuning to Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CjHof9FB0hM"
      },
      "outputs": [],
      "source": [
        "peft_config = PromptEncoderConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    num_virtual_tokens=30,\n",
        "    encoder_hidden_size=128\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClW7euQgB0hM",
        "outputId": "2e5eacd3-7eb6-4a5b-b3c7-0b64a87ccdee"
      },
      "outputs": [],
      "source": [
        "model = RobertaForSequenceClassification(model_config)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfquzX-WB0hN"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    'RoBerta-P-Tuning-Fine-Tuned',\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='no',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCH_COUNT,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoQs9n-eB0hN"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "s3gjoR3zB0hN",
        "outputId": "3eaebcd3-28ce-4eea-836e-3ffa4d7fd5cd"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(f'Elapsed time: {time.time() - start_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnsEcxcgB0hO"
      },
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNWwLlpyB0hO"
      },
      "source": [
        "### Part 3. Why LoRA?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealOeU2qB0hO"
      },
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX5ntNO6B0hP"
      },
      "source": [
        "### Part 1. In Context Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGgqRGqoB0hP"
      },
      "outputs": [],
      "source": [
        "LLAMA_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4y7bSNrB0hP"
      },
      "source": [
        "#### Zero Shot Prompting on llama 3B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMfMv40qB0hQ"
      },
      "source": [
        "We'll make a system prompt to tell the model what's our task and how it should behave."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duBNO1R7B0hQ"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You should get two sentences in every prompt. You will then say 'entailment' if two sentences entail eachother, 'neutral' if they might or might not entail eachother, and 'contradict' if they don't entail eachother.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMCF0TQ7B0hR"
      },
      "source": [
        "Gather some examples from the dataset and store their labels to evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIg1qalrB0hR"
      },
      "outputs": [],
      "source": [
        "examples = dataset['validation_matched'].shuffle(seed=42).select(range(10))\n",
        "labels = []\n",
        "\n",
        "for premise, hypothesis, label in zip(examples['premise'], examples['hypothesis'], examples['label']):\n",
        "    messages.append(\n",
        "        {\"role\": \"user\", \"content\": premise + \" \" + hypothesis}\n",
        "    )\n",
        "    labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmo6_ortB0hS"
      },
      "source": [
        "Now let's create our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsrWqu7EB0hS"
      },
      "outputs": [],
      "source": [
        "llama_zero_shot_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=LLAMA_MODEL_NAME,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VziH3KSRB0hS"
      },
      "source": [
        "The prompts should be in the way that llama3 has provided. We'll use the tokenizer in the pipeline to do so. Here is the format specified in the instructions [here](https://huggingface.co/blog/llama3#how-to-prompt-llama-3):\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{{ user_msg_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{ model_answer_1 }}<|eot_id|>s\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09bAU_b_B0hT"
      },
      "outputs": [],
      "source": [
        "prompt = llama_zero_shot_pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDHpnYj3B0hT"
      },
      "outputs": [],
      "source": [
        "terminators = [\n",
        "    llama_zero_shot_pipeline.tokenizer.eos_token_id,\n",
        "    llama_zero_shot_pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = llama_zero_shot_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwIAJddB0hU"
      },
      "source": [
        "#### Few Shot Prompting on llama 3B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re8TyrE5B0hU"
      },
      "source": [
        "We'll use the previous part's pipeline with a little modification. Instead of asking the model on example per prompt, and not giving it any example, we'll include 5 examples in each prompt: 2 from class 0, 1 from class 1, and 2 from class 2. We'll also tell the model about the new template in the system's prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjOfOKVJB0hU"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You should get 5 examples per each input. Each example contains two sentences followed by their label, indicating if the two sentences entail each other, they are neutral, or they contradict each other. After the 5 sentences there will be another example without a label. Predict the last example's label. Also note that 0 means 'entailment', 1 means 'neutral', and 2 means 'contradiction'\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K1pIWxkB0hV"
      },
      "outputs": [],
      "source": [
        "few_shot_0_examples = dataset['train'].filter(lambda x : x['label'] == 0).shuffle(seed=42).select(range(20))\n",
        "few_shot_1_examples = dataset['train'].filter(lambda x : x['label'] == 1).shuffle(seed=42).select(range(10))\n",
        "few_shot_2_examples = dataset['train'].filter(lambda x : x['label'] == 2).shuffle(seed=42).select(range(20))\n",
        "few_shot_eval_examples = dataset['validation_matched'].shuffle(seed=42).select(range(10))\n",
        "\n",
        "for i in range(10):\n",
        "    current_prompt = []\n",
        "\n",
        "    current_prompt.extend(\n",
        "        [\n",
        "            f\"{few_shot_0_examples['premise'][i*2]} {few_shot_0_examples['hypothesis'][i*2]} label={0}\\n\",\n",
        "            f\"{few_shot_0_examples['premise'][i*2 + 1]} {few_shot_0_examples['hypothesis'][i*2 + 1]} label={0}\\n\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    current_prompt.append(\n",
        "        f\"{few_shot_1_examples['premise'][i]} {few_shot_1_examples['hypothesis'][i]} label={1}\\n\",\n",
        "    )\n",
        "\n",
        "    current_prompt.extend(\n",
        "        [\n",
        "            f\"{few_shot_2_examples['premise'][i*2]} {few_shot_2_examples['hypothesis'][i*2]} label={2}\\n\",\n",
        "            f\"{few_shot_2_examples['premise'][i*2 + 1]} {few_shot_2_examples['hypothesis'][i*2 + 1]} label={2}\\n\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    current_prompt.append(f\"{few_shot_eval_examples['premise'][i]} {few_shot_eval_examples['hypothesis'][i]}\")\n",
        "\n",
        "    messages.append(\n",
        "        {\"role\": \"user\", \"content\": ''.join(current_prompt)}\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSioy_TqB0hV"
      },
      "outputs": [],
      "source": [
        "prompt = llama_zero_shot_pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj0BfNRVB0hW"
      },
      "outputs": [],
      "source": [
        "outputs = llama_zero_shot_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(outputs[0][\"generated_text\"][len(prompt):])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUtu5fGeB0hW"
      },
      "source": [
        "### Part 2. Fine Tuning the Model using QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgA-lCe4B0hX"
      },
      "source": [
        "First we'll use the sequence classifier model of llama and use QLoRA on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhtptBKiB0hX"
      },
      "outputs": [],
      "source": [
        "EPOCH_COUNT = 10\n",
        "LEARNING_RATE = 2e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCXXbZPOB0hX"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXKTAYtwB0hY"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
        "model = LlamaForSequenceClassification.from_pretrained(LLAMA_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XZ6-PDKB0hY"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cphhqWzJB0hZ"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbrjThqEB0hZ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    'Llama-QLoRA',\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCH_COUNT,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"no\",\n",
        "    optim=\"paged_adamw_8bit\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioTqVr2eB0hZ"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRWgizDrB0ha"
      },
      "outputs": [],
      "source": [
        "model = model.merge_and_unload()\n",
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
